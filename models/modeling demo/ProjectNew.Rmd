---
title: "Modelling Demo Caravan Insurance"
output: html_document
---

This is our project for Predictive Analytics 

# Project Overview

Following our initial meeting, we aligned on a business goal of **profitable growth** in the Caravan segment. The campaign will use personalized letters to existing customers and we will leverage predictive modeling to prioritize whom to contact.


```{r}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Business Inputs (from interview)

- **Budget (B):** €50,000  
- **Cost per contact (c):** €0.89 (letter)  
- **Revenue per new Caravan policy (R):** €600 p.a.  
- **Profit margin (m):** 5%  
- **Total addressable base:** 500,000 customers  
- **Random seed (reproducibility):** 42

## Derived KPIs (used throughout)

We’ll compute:
- **Profit per conversion:** \( p = R \times m \)  
- **Max contacts under budget:** \( N_{\text{budget}} = \left\lfloor \dfrac{B}{c} \right\rfloor \)  
- **Breakeven precision:** \( \text{Precision}_{\text{min}} = \dfrac{c}{p} \)  
  (minimum positive rate among contacted customers needed to avoid losses)

These KPIs translate model scores into actionable, **budget-constrained** outreach plans.

## Modeling Objective

Build a propensity model that ranks customers by likelihood to buy **Caravan**. We’ll compare operating strategies:

1. **Top-N (Budget-Constrained):** Contact the top \(N_{\text{budget}}\) customers by predicted probability.  
2. **Profit-Optimal Threshold:** Choose a probability cutoff that maximizes expected profit.

For both, we will report **Precision, Recall, Lift, AUC/PR-AUC, Profit, ROI**, and recommend the plan that best aligns with “**maximize growth while staying profitable**.”

## Data & Method (high level)

- **Dataset:** ticdata2000 (cleaned snapshot).  
- **Split:** 70% **training** / 30% **test** (seed = 42).  
- **Features:** Household-level predictors prioritized; highly correlated postcode prevalence variables dropped to reduce redundancy.  
- **Models:** Logistic Regression (baseline), Conditional Inference Tree (interpretable).  
- **Evaluation:** ROC/AUC, PR-AUC (for class imbalance), calibration (HL test + decile plot), and **business KPIs** under Top-N and profit-optimal threshold.

## Decision & Deployment

- Select the operating plan with **higher profit** (and acceptable precision).  
- **Pilot** (A/B) on a small slice (e.g., 2 weeks), verify conversion uplift and realized profit, then scale.

---

### Business inputs (code)
```{r}
marketing_cost       <- 0.89
marketing_budget     <- 50000
revenue_per_client   <- 600
profit_margin        <- 0.05
total_market_size    <- 500000
const_seed           <- 42

# Derived KPIs
profit_per_win <- revenue_per_client * profit_margin            # € per true positive
n_budget       <- floor(marketing_budget / marketing_cost)       # max letters
breakeven_prec <- marketing_cost / profit_per_win                # min precision to break even

cat(sprintf("Profit per policy: €%.2f | Max contacts: %d | Breakeven precision: %.3f\n",
            profit_per_win, n_budget, breakeven_prec))
```





```{r}

library(tidyverse)



# Load caTools package for data partitioning
library(caTools) 

# Import our data and assing it to churndt
churndt <- read.csv('/Users/alenslaskovs/Desktop/WHU/PA/GroupWork/MSc_Predictive_Analytics_HS25_Group_Assignment/data/processed/ticdata2000_clean.csv')

# Display the structure of the dataset

head(churndt, 10)

```

***

* Split the dataset into the training (70%) and test (30%) sets by using `sample.split()` function.

```{r  message=FALSE, eval=FALSE}

# Set a seed

const_seed <- 1234
set.seed(const_seed)

# Generate a vector partition for data partitioning 
partition = sample.split(churndt$CARAVAN, SplitRatio = 0.7) 

# Create training set: training
training_set = subset(churndt, partition == TRUE) 

# Create test set: test
test = subset(churndt, partition == FALSE) 

```

# Information Gain on training set

```{r}

install.packages("FSelectorRcpp")
library(FSelectorRcpp)

ig <- information_gain(CARAVAN ~ ., data = training_set)

# Sort descending and take top 15
ig <- arrange(ig, desc(importance))
print(head(ig, 20))  # peek at top 20 for context

top_k <- 15
selected_attrs <- ig$feature[seq_len(min(top_k, nrow(ig)))]
cat("Top attributes:\n"); print(selected_attrs)

# Build reduced training subset (selected predictors + target)
subset_data <- training_set[, c(selected_attrs, "CARAVAN")]

str(subset_data)
```


```{r}
# --- Inputs ---------------------------------------------------------------
shortlist <- c(
  "APERSAUT","ABRAND","AWAPART",     # household product holdings
  "PPERSAUT","PBRAND","PWAPART",     # postcode prevalence of those products
  "MINKGEM","MKOOPKLA",              # affluence / purchase power
  "MHKOOP","MOPLHOOG","MBERHOOG",    # housing, education, status
  "MOSTYPE"                          # nominal -> will be excluded if non-numeric
)

# --- 1) Select existing, numeric variables --------------------------------
vars_exist <- intersect(shortlist, names(training_set))
is_num     <- vapply(training_set[vars_exist], is.numeric, logical(1))
vars_num   <- vars_exist[is_num]

if (length(vars_num) < 2L) stop("Need at least 2 numeric variables for correlation.")

df_num <- training_set[, vars_num, drop = FALSE]

# Drop zero-variance columns
is_const <- vapply(df_num, function(x) isTRUE(var(x, na.rm = TRUE) == 0), logical(1))
if (any(is_const)) {
  message("Dropping zero-variance columns: ",
          paste(names(df_num)[is_const], collapse = ", "))
  df_num <- df_num[, !is_const, drop = FALSE]
}

if (ncol(df_num) < 2L) stop("After dropping constants, fewer than 2 variables remain.")

# --- 2) Spearman correlation matrix ---------------------------------------
cor_mat <- cor(df_num, method = "spearman", use = "pairwise.complete.obs")

# Optional: take a quick look
# print(round(cor_mat, 3))

# --- 3) Build upper-tri pair list -----------------------------------------
upper_idx <- which(upper.tri(cor_mat), arr.ind = TRUE)
pairs_df <- data.frame(
  var1 = rownames(cor_mat)[upper_idx[, 1]],
  var2 = colnames(cor_mat)[upper_idx[, 2]],
  rho  = cor_mat[upper_idx],
  stringsAsFactors = FALSE
)

# --- 4) Flag highly correlated pairs (abs rho ≥ threshold) -----------------
threshold <- 0.80
pairs_df$abs_rho <- abs(pairs_df$rho)

high_pairs <- pairs_df[pairs_df$abs_rho >= threshold, , drop = FALSE]
high_pairs <- high_pairs[order(-high_pairs$abs_rho), , drop = FALSE]

cat(sprintf("\nSpearman |rho| >= %.2f pairs:\n", threshold))
if (nrow(high_pairs)) {
  print(high_pairs, row.names = FALSE)
} else {
  cat("None found.\n")
}


# From your shortlist
shortlist <- c(
  "APERSAUT","ABRAND","AWAPART",
  "PPERSAUT","PBRAND","PWAPART",
  "MINKGEM","MKOOPKLA",
  "MHKOOP","MOPLHOOG","MBERHOOG",
  "MOSTYPE"
)

# Drop the postcode-prevalence (2nd) variables from the high-corr pairs
drop_vars <- c("PWAPART","PPERSAUT","PBRAND")
shortlist_pruned <- setdiff(shortlist, drop_vars)
```

So as discussed with the group we found out that the P variables are assigned with the ZIP codes so we decided to drop the var1 and keep the var2 as it will be easier to target the end-client with (marketing,etc)


Next, we will learn how to build some of the predictive models. Training data would be used to train and develop our predictive models. Later, we will use test data to predict customer class (1) for churn or (0) for stay. 



**Decision Tree, Logistic Regression & Naive Bayes**

Now, we will build logistic regression model 

install.packages("partykit")
install.packages("naivebayes")
install.packages("pROC")
install.packages("MASS")
install.packages("caret")

# Decision tree model ctree is part of package "partykit"
library(partykit)

# Naive Bayes model is part of package naivebayes
library(naivebayes)

# pROC chart library
library(pROC)

# LDA model is part of package MASS
library(MASS)


# Confusion Matrix Library
library(caret)

# features
features_used <- intersect(shortlist_pruned, names(training_set))

# formula
fT3 <- as.formula(paste("CARAVAN ~", paste(features_used, collapse = "+")))
columns <- c(features_used, "CARAVAN")

# Ensure target is factor and align test set
training_set$CARAVAN <- factor(training_set$CARAVAN)
test_set <- test
test_set$CARAVAN <- factor(test_set$CARAVAN, levels = levels(training_set$CARAVAN))

# Use your Caravan features
features_used <- intersect(shortlist_pruned, names(training_set))
fT3 <- as.formula(paste("CARAVAN ~", paste(features_used, collapse = "+")))
columns <- c(features_used, "CARAVAN")

# Fit models
decTree_T3     <- partykit::ctree(fT3, data = training_set[, columns])  # ← replace this line
logitModel_T3  <- glm(fT3, family = binomial("logit"), data = training_set[, columns])
NBmodel_T3     <- naive_bayes(fT3, data = training_set[, columns])
LDAmodel_T3    <- lda(fT3, data = training_set[, columns])

# Predict probabilities on test (needed for ROC)
p_dt  <- predict(decTree_T3, newdata = test_set, type = "prob")[, 2]      # P(CARAVAN=1)
p_lr  <- predict(logitModel_T3, newdata = test_set, type = "response")     # P(CARAVAN=1)
p_nb  <- predict(NBmodel_T3,  newdata = test_set, type = "prob")[, 2]      # P(CARAVAN=1)

# Actual labels
actual <- test_set$CARAVAN

# LDA probabilities for class "1"
lda_out <- predict(LDAmodel_T3, newdata = test_set)
p_lda   <- if (!is.null(colnames(lda_out$posterior)) && "1" %in% colnames(lda_out$posterior)) {
  lda_out$posterior[, "1"]
} else {
  lda_out$posterior[, 2]
}

# 3) Plot ROC curves (your format)
roc_decTree_T3 <- plot.roc(actual, p_dt,
main = "ROC: decision tree (red), logistic reg (blue), NB (green)",col = "red")
roc_logit_T3   <- lines.roc(actual, p_lr, col = "blue")
roc_NB_T3      <- lines.roc(actual, p_nb, col = "green")
lines.roc(actual, p_lda, col = "purple")





# DTree plot

plot(decTree, type = "simple")




# Actual default outcomes
actual <- test_set$default


# choose positive label (uses "1" if present; otherwise last level)
POS <- if ("1" %in% levels(test_set$CARAVAN)) "1" else tail(levels(test_set$CARAVAN), 1)
NEG <- setdiff(levels(test_set$CARAVAN), POS)[1]

to_class <- function(p) factor(ifelse(p >= 0.5, POS, NEG), levels = levels(test_set$CARAVAN))

pred_dt_class  <- to_class(p_dt)
pred_lr_class  <- to_class(p_lr)
pred_nb_class  <- to_class(p_nb)
pred_lda_class <- to_class(p_lda)

cm_dt  <- confusionMatrix(pred_dt_class,  test_set$CARAVAN, positive = POS)
cm_lr  <- confusionMatrix(pred_lr_class,  test_set$CARAVAN, positive = POS)
cm_nb  <- confusionMatrix(pred_nb_class,  test_set$CARAVAN, positive = POS)
cm_lda <- confusionMatrix(pred_lda_class, test_set$CARAVAN, positive = POS)

cm_dt; cm_lr; cm_nb; cm_lda


precision <- as.numeric(cm_dt$byClass["Pos Pred Value"])  # = Precision
recall    <- as.numeric(cm_dt$byClass["Sensitivity"])     # = Recall/TPR
fpr       <- 1 - as.numeric(cm_dt$byClass["Specificity"]) # = FPR
f1        <- 2 * precision * recall / (precision + recall)
c(precision=precision, recall=recall, fpr=fpr, f1=f1)


precision <- as.numeric(cm_lr$byClass["Pos Pred Value"])  # = Precision
recall    <- as.numeric(cm_lr$byClass["Sensitivity"])     # = Recall/TPR
fpr       <- 1 - as.numeric(cm_lr$byClass["Specificity"]) # = FPR
f1        <- 2 * precision * recall / (precision + recall)
c(precision=precision, recall=recall, fpr=fpr, f1=f1)

precision <- as.numeric(cm_nb$byClass["Pos Pred Value"])  # = Precision
recall    <- as.numeric(cm_nb$byClass["Sensitivity"])     # = Recall/TPR
fpr       <- 1 - as.numeric(cm_nb$byClass["Specificity"]) # = FPR
f1        <- 2 * precision * recall / (precision + recall)
c(precision=precision, recall=recall, fpr=fpr, f1=f1)

precision <- as.numeric(cm_lda$byClass["Pos Pred Value"])  # = Precision
recall    <- as.numeric(cm_lda$byClass["Sensitivity"])     # = Recall/TPR
fpr       <- 1 - as.numeric(cm_lda$byClass["Specificity"]) # = FPR
f1        <- 2 * precision * recall / (precision + recall)
c(precision=precision, recall=recall, fpr=fpr, f1=f1)




