---
title: "Modelling Demo Caravan Insurance"
output: html_document
---

This is our project for Predictive Analytics 

# Project Overview

Following our initial meeting, we aligned on a business goal of **profitable growth** in the Caravan segment. The campaign will use personalized letters to existing customers and we will leverage predictive modeling to prioritize whom to contact.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Business Inputs (from interview)

- **Budget (B):** €50,000  
- **Cost per contact (c):** €0.89 (letter)  
- **Revenue per new Caravan policy (R):** €600 p.a.  
- **Profit margin (m):** 5%  
- **Total addressable base:** 500,000 customers  
- **Random seed (reproducibility):** 42

## Derived KPIs (used throughout)

We’ll compute:
- **Profit per conversion:** \( p = R \times m \)  
- **Max contacts under budget:** \( N_{\text{budget}} = \left\lfloor \dfrac{B}{c} \right\rfloor \)  
- **Breakeven precision:** \( \text{Precision}_{\text{min}} = \dfrac{c}{p} \)  
  (minimum positive rate among contacted customers needed to avoid losses)

These KPIs translate model scores into actionable, **budget-constrained** outreach plans.

## Modeling Objective

Build a propensity model that ranks customers by likelihood to buy **Caravan**. We’ll compare operating strategies:

1. **Top-N (Budget-Constrained):** Contact the top \(N_{\text{budget}}\) customers by predicted probability.  
2. **Profit-Optimal Threshold:** Choose a probability cutoff that maximizes expected profit.

For both, we will report **Precision, Recall, Lift, AUC/PR-AUC, Profit, ROI**, and recommend the plan that best aligns with “**maximize growth while staying profitable**.”

## Data & Method (high level)

- **Dataset:** ticdata2000 (cleaned snapshot).  
- **Split:** 70% **training** / 30% **test** (seed = 42).  
- **Features:** Household-level predictors prioritized; highly correlated postcode prevalence variables dropped to reduce redundancy.  
- **Models:** Logistic Regression (baseline), Conditional Inference Tree (interpretable).  
- **Evaluation:** ROC/AUC, PR-AUC (for class imbalance), calibration (HL test + decile plot), and **business KPIs** under Top-N and profit-optimal threshold.

## Decision & Deployment

- Select the operating plan with **higher profit** (and acceptable precision).  
- **Pilot** (A/B) on a small slice (e.g., 2 weeks), verify conversion uplift and realized profit, then scale.

---

### Business inputs (code)
```{r}
marketing_cost       <- 0.89
marketing_budget     <- 50000
revenue_per_client   <- 600
profit_margin        <- 0.05
total_market_size    <- 500000
const_seed           <- 42

# Derived KPIs
profit_per_win <- revenue_per_client * profit_margin            # € per true positive
n_budget       <- floor(marketing_budget / marketing_cost)       # max letters
breakeven_prec <- marketing_cost / profit_per_win                # min precision to break even

cat(sprintf("Profit per policy: €%.2f | Max contacts: %d | Breakeven precision: %.3f\n",
            profit_per_win, n_budget, breakeven_prec))
```

### Business KPIs after scoring (place this **after** you create `p_lr` and `test`)
```{r}
# Requires: p_lr (probabilities on test), test$CARAVAN defined earlier

y <- as.integer(test$CARAVAN == "1")

metrics_at <- function(pred_idx) {
  TP <- sum(y[pred_idx] == 1); FP <- length(pred_idx) - TP
  spend    <- (TP + FP) * marketing_cost
  profit   <- TP * profit_per_win - spend
  precision <- ifelse(TP+FP == 0, NA, TP/(TP+FP))
  recall    <- TP / sum(y)
  roi       <- ifelse(spend == 0, NA, profit / spend)
  c(N=length(pred_idx), TP=TP, FP=FP, Precision=precision, Recall=recall,
    Profit=profit, ROI=roi, Spend=spend)
}

# Plan A: Top-N (budget)
ord  <- order(p_lr, decreasing = TRUE)
pick <- ord[seq_len(min(n_budget, length(ord)))]
res_topN <- metrics_at(pick)
cat("\nTop-N (budget) results:\n"); print(round(res_topN, 3))

# Plan B: Profit-optimal threshold scan
grid <- seq(0.01, 0.50, by = 0.005)
scan_res <- sapply(grid, function(th) {
  pred_idx <- which(p_lr >= th)
  metrics_at(pred_idx)["Profit"]
})
best_th <- grid[which.max(scan_res)]
pred_idx_best <- which(p_lr >= best_th)
res_best <- metrics_at(pred_idx_best)

cat(sprintf("\nProfit-opt threshold: th = %.3f\n", best_th))
print(round(res_best, 3))

# Compact comparison
comp <- rbind(TopN = res_topN, ProfitOpt = res_best)
as.data.frame(t(round(comp, 3)))
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)
```


```{r  message=FALSE}

library(tidyverse)



# Load caTools package for data partitioning
library(caTools) 

# Import our data and assing it to churndt
churndt <- read.csv('/Users/alenslaskovs/Desktop/WHU/PA/GroupWork/MSc_Predictive_Analytics_HS25_Group_Assignment/data/processed/ticdata2000_clean.csv')

# Display the structure of the dataset

head(churndt, 10)

```

Let's start with data preparation.

* Customer ID (id) does not have any effect on the target variable. Remove that feature from the dataset.

```{r  message=FALSE}

# Remove customer ID from the dataset

#churndt <- subset(churndt, select = -id)

```

* Display the summary of the dataset. 

```{r  message=FALSE}

# Display the summary
str(churndt)

```

***

* Split the dataset into the training (70%) and test (30%) sets by using `sample.split()` function.

```{r  message=FALSE, eval=FALSE}

# Set a seed

const_seed <- 1234
set.seed(const_seed)

# Generate a vector partition for data partitioning 
partition = sample.split(churndt$CARAVAN, SplitRatio = 0.7) 

# Create training set: training
training_set = subset(churndt, partition == TRUE) 

# Create test set: test
test = subset(churndt, partition == FALSE) 

```


# Information Gain on training set

```{r}

install.packages("FSelectorRcpp")
library(FSelectorRcpp)

ig <- information_gain(CARAVAN ~ ., data = training_set)

# Sort descending and take top 15
ig <- arrange(ig, desc(importance))
print(head(ig, 20))  # peek at top 20 for context

top_k <- 15
selected_attrs <- ig$feature[seq_len(min(top_k, nrow(ig)))]
cat("Top attributes:\n"); print(selected_attrs)

# Build reduced training subset (selected predictors + target)
subset_data <- training_set[, c(selected_attrs, "CARAVAN")]

str(subset_data)
```


# --- Inputs ---------------------------------------------------------------
shortlist <- c(
  "APERSAUT","ABRAND","AWAPART",     # household product holdings
  "PPERSAUT","PBRAND","PWAPART",     # postcode prevalence of those products
  "MINKGEM","MKOOPKLA",              # affluence / purchase power
  "MHKOOP","MOPLHOOG","MBERHOOG",    # housing, education, status
  "MOSTYPE"                          # nominal -> will be excluded if non-numeric
)

# --- 1) Select existing, numeric variables --------------------------------
vars_exist <- intersect(shortlist, names(training_set))
is_num     <- vapply(training_set[vars_exist], is.numeric, logical(1))
vars_num   <- vars_exist[is_num]

if (length(vars_num) < 2L) stop("Need at least 2 numeric variables for correlation.")

df_num <- training_set[, vars_num, drop = FALSE]

# Drop zero-variance columns
is_const <- vapply(df_num, function(x) isTRUE(var(x, na.rm = TRUE) == 0), logical(1))
if (any(is_const)) {
  message("Dropping zero-variance columns: ",
          paste(names(df_num)[is_const], collapse = ", "))
  df_num <- df_num[, !is_const, drop = FALSE]
}

if (ncol(df_num) < 2L) stop("After dropping constants, fewer than 2 variables remain.")

# --- 2) Spearman correlation matrix ---------------------------------------
cor_mat <- cor(df_num, method = "spearman", use = "pairwise.complete.obs")

# Optional: take a quick look
# print(round(cor_mat, 3))

# --- 3) Build upper-tri pair list -----------------------------------------
upper_idx <- which(upper.tri(cor_mat), arr.ind = TRUE)
pairs_df <- data.frame(
  var1 = rownames(cor_mat)[upper_idx[, 1]],
  var2 = colnames(cor_mat)[upper_idx[, 2]],
  rho  = cor_mat[upper_idx],
  stringsAsFactors = FALSE
)

# --- 4) Flag highly correlated pairs (abs rho ≥ threshold) -----------------
threshold <- 0.80
pairs_df$abs_rho <- abs(pairs_df$rho)

high_pairs <- pairs_df[pairs_df$abs_rho >= threshold, , drop = FALSE]
high_pairs <- high_pairs[order(-high_pairs$abs_rho), , drop = FALSE]

cat(sprintf("\nSpearman |rho| >= %.2f pairs:\n", threshold))
if (nrow(high_pairs)) {
  print(high_pairs, row.names = FALSE)
} else {
  cat("None found.\n")
}

#So as discussed with the group we found out that the P variables are assigned with the ZIP codes so we decided to drop the var1 and keep the var2 as it will be easier to target the end-client with (marketing,etc)


# From your shortlist
shortlist <- c(
  "APERSAUT","ABRAND","AWAPART",
  "PPERSAUT","PBRAND","PWAPART",
  "MINKGEM","MKOOPKLA",
  "MHKOOP","MOPLHOOG","MBERHOOG",
  "MOSTYPE"
)

# Drop the postcode-prevalence (2nd) variables from the high-corr pairs
drop_vars <- c("PWAPART","PPERSAUT","PBRAND")
shortlist_pruned <- setdiff(shortlist, drop_vars)


***

Next, we will learn how to build some of the predictive models. Training data would be used to train and develop our predictive models. Later, we will use test data to predict customer class (1) for churn or (0) for stay. 



**Logistic Regression Model**

Now, we will build logistic regression model 



logistic_model <- glm(formula, data = train_data, family = binomial(link = "logit"))
summary(logistic_model)



# --- Packages ---------------------------------------------------------------
if (!requireNamespace("pROC", quietly = TRUE)) install.packages("pROC")
if (!requireNamespace("caret", quietly = TRUE)) install.packages("caret")
if (!requireNamespace("car", quietly = TRUE)) install.packages("car")
library(pROC)
library(caret)
library(car)

# --- Target & types ---------------------------------------------------------
# Ensure target is factor with levels 0/1 (GLM uses 0/1 coding)
training_set$CARAVAN <- as.factor(training_set$CARAVAN)
test$CARAVAN        <- as.factor(test$CARAVAN)

# MOSTYPE should be nominal (factor) if present
if ("MOSTYPE" %in% names(training_set) && !is.factor(training_set$MOSTYPE)) {
  training_set$MOSTYPE <- as.factor(training_set$MOSTYPE)
  test$MOSTYPE        <- as.factor(test$MOSTYPE)
}

# --- Build formula from pruned features ------------------------------------
feat <- intersect(shortlist_pruned, names(training_set))
form_lr <- as.formula(paste("CARAVAN ~", paste(feat, collapse = " + ")))
form_lr

logit_mod <- glm(form_lr, data = training_set, family = binomial())

summary(logit_mod)

# Multicollinearity check (optional)
vif_vals <- car::vif(logit_mod)
print(vif_vals)


# --- Logistic ROC only ------------------------------------------------------
# Probabilities for class "1"
p_lr <- predict(logit_mod, newdata = test, type = "response")

# Ensure positive class is "1"
actual <- factor(test$CARAVAN)  # levels should be c("0","1")
roc_lr <- pROC::roc(response = actual, predictor = p_lr, levels = rev(levels(actual)))
auc_lr <- pROC::auc(roc_lr)
cat(sprintf("Logistic AUC: %.3f\n", as.numeric(auc_lr)))

# Plot ROC
plot.roc(roc_lr, legacy.axes = TRUE, col = "blue", lwd = 2,
         xlab = "False Positive Rate (1 - Specificity)",
         ylab = "True Positive Rate (Sensitivity)",
         main = "ROC — Logistic Regression")
abline(a = 0, b = 1, lty = 2)

# Mark threshold = 0.50
pt_05 <- pROC::coords(roc_lr, x = 0.5, input = "threshold",
                      ret = c("sensitivity","specificity"))
points(1 - pt_05["specificity"], pt_05["sensitivity"], pch = 19)
text(1 - pt_05["specificity"], pt_05["sensitivity"], labels = "  th=0.50", pos = 4)

# Mark Youden-optimal threshold
pt_best <- pROC::coords(roc_lr, x = "best", best.method = "youden",
                        ret = c("threshold","sensitivity","specificity"))
points(1 - pt_best["specificity"], pt_best["sensitivity"], pch = 19)
text(1 - pt_best["specificity"], pt_best["sensitivity"],
     labels = sprintf("  th=%.3f", as.numeric(pt_best["threshold"])), pos = 4)

legend("bottomright",
       legend = sprintf("Logistic Reg. (AUC = %.3f)", as.numeric(auc_lr)),
       col = "blue", lwd = 2, bty = "n")



**Decision Tree**

Now, we will build one of the decision tree models known as conditional inference tree in R. The function `ctree()` is used to create conditional trees. The main components of this function are formula (specifying which is the target variable and what  features will be used in modelling) and data. The basic syntax of this function can be given as follows:

    ctree(formula, data)

`ctree()` function is located in package `partykit`. Let's load `partykit` package and start to work on the decision tree model. 

```{r  message=FALSE, eval = FALSE}

# Install party package
install.packages("partykit")

# Load package party
library(partykit)

```

* Build a decision tree by using all features. Call this model *decTree*. We will use this model for prediction.

```{r  message=FALSE, eval = FALSE}

# Build a decision tree by using ctree() function
decTree <- partykit::ctree(as.formula(paste("CARAVAN ~", paste(shortlist_pruned, collapse = " + "))), data = training_set)

prop.table(table(training_set$CARAVAN))


# Print decTree

print(decTree)


# --- 0) Packages ------------------------------------------------------------
if (!requireNamespace("partykit", quietly = TRUE)) install.packages("partykit")
library(partykit)

# --- 1) Variables (your shortlist + pruning) --------------------------------
shortlist <- c(
  "APERSAUT","ABRAND","AWAPART",
  "PPERSAUT","PBRAND","PWAPART",
  "MINKGEM","MKOOPKLA",
  "MHKOOP","MOPLHOOG","MBERHOOG",
  "MOSTYPE"
)
drop_vars <- c("PWAPART","PPERSAUT","PBRAND")
shortlist_pruned <- setdiff(shortlist, drop_vars)

# Keep only those that exist in training_set
feat <- intersect(shortlist_pruned, names(training_set))

# If MOSTYPE is present and not a factor, coerce (nominal)
if ("MOSTYPE" %in% feat && !is.factor(training_set$MOSTYPE)) {
  training_set$MOSTYPE <- as.factor(training_set$MOSTYPE)
}
if ("MOSTYPE" %in% names(test) && !is.factor(test$MOSTYPE)) {
  test$MOSTYPE <- as.factor(test$MOSTYPE)
}

# --- 2) Build & fit the tree ------------------------------------------------
form_pruned <- as.formula(paste("CARAVAN ~", paste(feat, collapse = " + ")))
decTree <- partykit::ctree(form_pruned, data = training_set)

# Class balance in training
cat("Training class balance (proportions):\n")
print(prop.table(table(training_set$CARAVAN)))

# Print model summary
cat("\nModel summary:\n")
print(decTree)

# --- 3) Tree size & depth ---------------------------------------------------
# number of terminal (leaf) nodes
n_leaves <- length(partykit::nodeids(decTree, from = "terminal"))
# number of inner (split) nodes
n_inner  <- length(partykit::nodeids(decTree, from = "inner"))

# helper to compute maximum depth (height) of the tree
tree_depth <- function(tree) {
  recurse <- function(nd) {
    kids <- partykit::kids_node(nd)
    if (length(kids) == 0L) return(1L)
    1L + max(vapply(kids, recurse, integer(1L)))
  }
  recurse(partykit::node_party(tree))
}

cat("\nTree stats:\n")
cat("Leaves:", n_leaves, " | Inner nodes:", n_inner, " | Max depth (nodes):", tree_depth(decTree), "\n")

# --- 4) Quick evaluation on test -------------------------------------------
pred <- predict(decTree, newdata = test, type = "response")
acc  <- mean(pred == test$CARAVAN)
cat(sprintf("\nTest accuracy: %.3f\n", acc))

# Optional: simple plot
plot(decTree, type = "simple")

```


* Build another decision tree by using features HOUSE and AVG_OVERCHARGE. We will use this model just for illustration.




* Use the *decTree* model and predict the classes in the test data using `predict()` function. 

  The syntax of `predict()` function is of the form `predict(model, testdata, type = "--")`. The first argument is the model object - technically a list - produced by a modellig functions (such as the object `DecTree` produced above). The input `testdata` denotes the testing dataset to be scored. The function takes one additional argument which is `type`. Type refers to the type of predicted value returned. For `ctree()`, type argument can take one of the following values:

  - response: the predicted class for a categorical response
  - prob: class probabilities
  - node: terminal nodes

```{r  message=FALSE, eval = FALSE}

# Predicting the Test set results 
 decTree_predict = predict(decTree, test, type = "response")


```


Let's find out how many of the predictions are correct. 

 * Copy test data to *results*

```{r message=FALSE, eval = FALSE}
 # Copy test data to results
results <- test
```

* Find the correct predictions.

```{r  message=FALSE, eval = FALSE}

#  Generate a column named PredictionTree in dataframe results and add predictions obtained by decision tree to that column
results$PredictionTree <- decTree_predict

# Find the correct predictions
correct_Tree <- which(results$PredictionTree == results$CARAVAN)

# Return the total number of correct predictions
length(correct_Tree)

```




**Linear Discriminant Analysis**

Linear Discriminant analysis (LDA) is a linear classification method. This method finds the linear combination of the attributes that separates the classes of target variable. To build an LDA model, we will use `lda()` function from `MASS` package. The basic syntax of this function can be given as follows:

      lda(formula, data)
      
Let's load `MASS` package and build an LDA model by using LEFTOVER, HOUSE, AVG_OVERCHARGE and AVG_CALL features. Call this model *LDAmodel*. 

```{r  message=FALSE, eval = FALSE}

# Load MASS package
library(MASS)

# Build an LDA model by using lda() function
LDAmodel <- lda(CLASS ~ ---, data = training)

# Print model
LDAmodel

```

* Predict the classes in the test data using `predict()` function. The syntax is:

              predict(modelname, test data)

```{r  message=FALSE, eval = FALSE}

# Predict the Test set results 
 LDA_predict = predict(LDAmodel, newdata = test)$class

```

* By following the same steps as the decision tree model, find the correct predictions.

```{r  message=FALSE, eval = FALSE}

#  Generate a column named PredictionLDA in dataframe results and add predictions obtained by LDA to that column
results$PredictionLDA <- predict(LDAmodel, newdata = results)$class

# Find the correct predictions
correct_LDA <- which(results$PredictionLDA == results$CLASS)

# Return the total number of correct predictions
length(correct_LDA)

```







**Support Vector Machine (SVM)**

The function `svm()` in package `e1071` is used to develop a SVM model. The basic syntax of this function can be given as follows

    svm(formula, data, kernel method, scale=TRUE )
    
- Formula shows which features are used in modelling to predict the target variable.

- Data is the dataset that will be used for model building.

- If our data is linearly separable, we use linear separation `kernel = "linear"`. However, if our data is not linearly separable, then we should use other separation techniques (such as `kernel = "radial"`). For other kernel techniques, you can check [R documentation](https://www.rdocumentation.org/packages/e1071/versions/1.7-2/topics/svm). 

- The last argument `scale=TRUE` is related to feature scaling. When one feature 
has very large values, it will dominate the other features when calculating the distance in SVM. Therefore, we scale them to have equal influence on the SVM metrics.

Let's load `e1071` package and start to work on the SVM model.

```{r  message=FALSE, eval = FALSE}

# Install e1071 package for SVM model
install.packages("e1071")

# Load package e1071
library(e1071)

```

***

* Build an SVM model by using all features. Set the `kernel = "radial"`. Call this model *svm_radial*.

```{r  message=FALSE, eval = FALSE}

# Build an SVM model by using svm() function
svm_radial <- svm(CLASS ~ ., data = training_set, kernel = "radial", scale = TRUE)

# Print svm_radial
print(svm_radial)

```

* Using this model, predict customer classification (1 or 0) for the test data. Use `predict()` function for this task. The basic syntax of `predict()` function for SVM takes two arguments: predictive model (in this case svm_radial) and unseen dataset which is our test set. The syntax is:

         predict(modelname, test data)

```{r  message=FALSE, eval=FALSE}

# Predicting the Test set results 
 svm_predict <- predict(svm_radial, newdata = test)

```


Let's find out how many of the predictions are correct. 

 * Copy test data to *results*  
 * Generate a column in *results* as "PredictionSVM" and add predictions obtained by SVM.
 
```{r  message=FALSE, eval=FALSE}

# Copy test data to results
results <- test

# The following code generates a column named PredictionSVM in dataframe results and adds predictions obtained by SVM to that column
results$PredictionSVM <- predict(svm_radial, newdata = results)

```
 
 * Find the correct predictions by comparing "CLASS" and "PredictionSVM" columns.

```{r  message=FALSE, eval = FALSE}

# Find the correct predictions
correct_svm <- which(results$PredictionSVM == results$CLASS)

# Return the total number of correct predictions by using length() function
length(correct_svm)

```





