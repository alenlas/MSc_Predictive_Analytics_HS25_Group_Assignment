---
title: "Modelling Demo Caravan Insurance"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

This is our project for Predictive Analytics

# Project Overview

Following our initial meeting, we aligned on a business goal of
**profitable growth** in the Caravan segment. The campaign will use
personalized letters to existing customers and we will leverage
predictive modeling to prioritize whom to contact.

```{r}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Business Inputs (from interview)

-   **Budget (B):** €50,000\
-   **Cost per contact (c):** €0.89 (letter)\
-   **Revenue per new Caravan policy (R):** €600 p.a.\
-   **Profit margin (m):** 5%\
-   **Total addressable base:** 500,000 customers\
-   **Random seed (reproducibility):** 42

## Derived KPIs (used throughout)

We’ll compute: - **Profit per conversion:** $p = R \times m$\
- **Max contacts under budget:**
$N_{\text{budget}} = \left\lfloor \dfrac{B}{c} \right\rfloor$\
- **Breakeven precision:**
$\text{Precision}_{\text{min}} = \dfrac{c}{p}$\
(minimum positive rate among contacted customers needed to avoid losses)

These KPIs translate model scores into actionable,
**budget-constrained** outreach plans.

## Modeling Objective

Build a propensity model that ranks customers by likelihood to buy
**Caravan**. We’ll compare operating strategies:

1.  **Top-N (Budget-Constrained):** Contact the top $N_{\text{budget}}$
    customers by predicted probability.\
2.  **Profit-Optimal Threshold:** Choose a probability cutoff that
    maximizes expected profit.

For both, we will report **Precision, Recall, Lift, AUC/PR-AUC, Profit,
ROI**, and recommend the plan that best aligns with “**maximize growth
while staying profitable**.”

## Data & Method (high level)

-   **Dataset:** ticdata2000 (cleaned snapshot).\
-   **Split:** 70% **training** / 30% **test** (seed = 1234).\
-   **Features:** Household-level predictors prioritized; highly
    correlated postcode prevalence variables dropped to reduce
    redundancy.\
-   **Models:** Logistic Regression (baseline), Conditional Inference
    Tree (interpretable).\
-   **Evaluation:** ROC/AUC, PR-AUC (for class imbalance), calibration
    (HL test + decile plot), and **business KPIs** under Top-N and
    profit-optimal threshold.

## Decision & Deployment

-   Select the operating plan with **higher profit** (and acceptable
    precision).\
-   **Pilot** (A/B) on a small slice (e.g., 2 weeks), verify conversion
    uplift and realized profit, then scale.

------------------------------------------------------------------------

### Business inputs (code)

```{r}
marketing_cost       <- 0.89
marketing_budget     <- 50000
revenue_per_client   <- 600
profit_margin        <- 0.05
total_market_size    <- 500000
const_seed           <- 42

# Derived KPIs
profit_per_win <- revenue_per_client * profit_margin            # € per true positive
n_budget       <- floor(marketing_budget / marketing_cost)       # max letters
breakeven_prec <- marketing_cost / profit_per_win                # min precision to break even

cat(sprintf("Profit per policy: €%.2f | Max contacts: %d | Breakeven precision: %.3f\n",
            profit_per_win, n_budget, breakeven_prec))
```

```{r}

library(tidyverse)
library(caTools) 

# Import our data and assing it to churndt
churndt <- read.csv("ticdata2000_clean.csv")

#churndt <- read.csv("C:/Users/antealocal/OneDrive - WHU/Q1/Predictive Analytics/Assessment/ticdata2000_clean.csv")

# Display the structure of the dataset

head(churndt, 10)

# target as factor with readable labels
churndt$CARAVAN <- factor(churndt$CARAVAN, levels = c(0,1), labels = c("no","yes"))

# MOSTYPE: treat as categorical and collapse rare levels (prettier trees)
churndt$MOSTYPE <- fct_lump_n(factor(churndt$MOSTYPE), n = 10)

```

------------------------------------------------------------------------

-   Split the dataset into the training (70%) and test (30%) sets by
    using `sample.split()` function.

```{r}

# Set a seed

const_seed <- 1234
set.seed(const_seed)

# Generate a vector partition for data partitioning 
partition = sample.split(churndt$CARAVAN, SplitRatio = 0.7) 

# Create training set: training
training_set = subset(churndt, partition == TRUE) 

# Create test set: test
test = subset(churndt, partition == FALSE) 

```

# Balancing the dataset

DO WE NEED TO BALANCE THE DATASET? Data balancing (through oversampling
or undersampling) could be done to address class imbalance. In our case
one class CARAVAN occurs far less frequently than NO CARAVAN.

# Information Gain on training set

```{r}

library(FSelectorRcpp)

ig <- information_gain(CARAVAN ~ ., data = training_set)

# Sort descending and take top 15
ig <- arrange(ig, desc(importance))
print(head(ig, 20))  # peek at top 20 for context

top_k <- 15
selected_attrs <- ig$feature[seq_len(min(top_k, nrow(ig)))]
cat("Top attributes:\n"); print(selected_attrs)

# Build reduced training subset (selected predictors + target)
subset_data <- training_set[, c(selected_attrs, "CARAVAN")]

str(subset_data)
```

```{r Variable choice}
# This are our variables that we picked based on information gain
shortlist <- c(
  "APERSAUT","ABRAND","AWAPART",     # household product holdings
  "PPERSAUT","PBRAND","PWAPART",     # postcode prevalence of those products
  "MINKGEM","MKOOPKLA",              # affluence / purchase power
  "MHKOOP","MOPLHOOG","MBERHOOG",    # housing, education, status
  "MOSTYPE"                          # nominal -> will be excluded if non-numeric
)
```

IS THIS REALLY A GOOD CHOICE???????

```{r}
# select numeric variables
vars_exist <- intersect(shortlist, names(training_set))
is_num     <- vapply(training_set[vars_exist], is.numeric, logical(1))
vars_num   <- vars_exist[is_num]

if (length(vars_num) < 2L) stop("Need at least 2 numeric variables for correlation.")

df_num <- training_set[, vars_num, drop = FALSE]

# Drop zero-variance columns
is_const <- vapply(df_num, function(x) isTRUE(var(x, na.rm = TRUE) == 0), logical(1))
if (any(is_const)) {
  message("Dropping zero-variance columns: ",
          paste(names(df_num)[is_const], collapse = ", "))
  df_num <- df_num[, !is_const, drop = FALSE]
}

if (ncol(df_num) < 2L) stop("After dropping constants, fewer than 2 variables remain.")

# Spearman correlation matrix
cor_mat <- cor(df_num, method = "spearman", use = "pairwise.complete.obs")

print(round(cor_mat, 3))

# Build upper-tri pair list
upper_idx <- which(upper.tri(cor_mat), arr.ind = TRUE)
pairs_df <- data.frame(
  var1 = rownames(cor_mat)[upper_idx[, 1]],
  var2 = colnames(cor_mat)[upper_idx[, 2]],
  rho  = cor_mat[upper_idx],
  stringsAsFactors = FALSE
)

# Flag highly correlated pairs (abs rho ≥ threshold) 
threshold <- 0.80
pairs_df$abs_rho <- abs(pairs_df$rho)

high_pairs <- pairs_df[pairs_df$abs_rho >= threshold, , drop = FALSE]
high_pairs <- high_pairs[order(-high_pairs$abs_rho), , drop = FALSE]

cat(sprintf("\nSpearman |rho| >= %.2f pairs:\n", threshold))
if (nrow(high_pairs)) {
  print(high_pairs, row.names = FALSE)
} else {
  cat("None found.\n")
}



# Drop the postcode-prevalence (2nd) variables from the high-corr pairs
drop_vars <- c("PWAPART","PPERSAUT","PBRAND")
shortlist_pruned <- setdiff(shortlist, drop_vars)
```

So as discussed with the group we found out that the P variables are
assigned with the ZIP codes so we decided to drop the var1 and keep the
var2 as it will be easier to target the end-client with (marketing,etc)

Next, we will learn how to build some of the predictive models. Training
data would be used to train and develop our predictive models. Later, we
will use test data to predict customer class (1) for churn or (0) for
stay.

# (New) Decision Tree and Evaluation

```{r Packages}
library(partykit) #decision tree model ctree
library(naivebayes) #NaiveBayes model
library(pROC) #pROC chart library
library(MASS) #LDA model
library(caret) #Confusion matrix library   
```

```{r Features}
features_used <- intersect(shortlist_pruned, names(training_set))
fT3 <- as.formula(paste("CARAVAN ~", paste(features_used, collapse = "+")))
columns <- c(features_used, "CARAVAN")

```

```{r Ensure target is factor and align test set}
training_set$CARAVAN <- factor(training_set$CARAVAN)
test_set <- test
test_set$CARAVAN <- factor(test_set$CARAVAN, levels = levels(training_set$CARAVAN))
```

```{r Decision Tree T2}
# ---- cost-sensitive, compact ctree ---------------------------------------
# weight positives to counter class imbalance
pos_lab <- "yes"
w <- ifelse(training_set$CARAVAN == pos_lab,
            sum(training_set$CARAVAN != pos_lab) / sum(training_set$CARAVAN == pos_lab),
            1)

ctrl <- ctree_control(
  maxdepth    = 4,     # keeps the tree legible
  minsplit    = 200,   # avoid tiny, noisy splits
  minbucket   = 100,
  mincriterion= 0.90   # allow a bit more splitting than the 0.95 default
)

decTree_T2 <- ctree(fT3, data = training_set[, columns], weights = w, control = ctrl)

# Plot
library(grid)
plot(decTree_T2,
     inner_panel    = node_inner(decTree_T2, id = FALSE, pval = TRUE),
     terminal_panel = node_terminal(decTree_T2, digits = 2, abbreviate = FALSE, id = FALSE),
     gp = gpar(fontsize = 8))
```

```{r Evaluation}

library(caret)
library(pROC)

# business cutoff + profit-maximizing threshold 
# Assumptions 
profit_per_conv <- 30      # € profit per conversion
cost_mail       <- 0.85    # € cost per mailing
cutoff          <- cost_mail / profit_per_conv  # break-even prob ≈ 0.0283
pos_lab         <- "yes"   # positive class label in test$CARAVAN

# Score the test set with your fitted tree (decTree_T2) 
p_dt2 <- predict(decTree_T2, newdata = test, type = "prob")[, pos_lab]

# Metrics at business break-even cutoff 
pred_cut <- factor(ifelse(p_dt2 >= cutoff, pos_lab, "no"),
                   levels = levels(test$CARAVAN))
cm_cut   <- confusionMatrix(pred_cut, test$CARAVAN, positive = pos_lab)
auc_dt2  <- auc(response = test$CARAVAN, predictor = p_dt2)

profit_cut <- sum(ifelse(p_dt2 >= cutoff,
                         ifelse(test$CARAVAN == pos_lab,
                                profit_per_conv - cost_mail, -cost_mail),
                         0))

# * Profit-maximizing threshold over a simple grid 
ths <- sort(unique(c(0, quantile(p_dt2, probs = seq(0, 1, 0.01)))))

profit_of <- function(t){
  mail <- p_dt2 >= t
  sum(ifelse(mail,
             ifelse(test$CARAVAN == pos_lab, profit_per_conv - cost_mail, -cost_mail),
             0))
}

profits <- sapply(ths, profit_of)
t_star  <- ths[which.max(profits)]

pred_star <- factor(ifelse(p_dt2 >= t_star, pos_lab, "no"),
                    levels = levels(test$CARAVAN))
cm_star   <- confusionMatrix(pred_star, test$CARAVAN, positive = pos_lab)
profit_star <- max(profits)

```

```{r Summary table}
# Full summary table 
library(dplyr)
library(tibble)
library(knitr)

# Formatters
fmt_pct <- function(x) sprintf("%.2f%%", 100 * as.numeric(x))
fmt_eur <- function(x) sprintf("€%s", formatC(as.numeric(x), format = "f", digits = 2, big.mark = ","))
fmt_int <- function(x) format(as.integer(x), big.mark = ",")

# Counts & rates
mailed_cut  <- sum(p_dt2 >= cutoff)
mailed_star <- sum(p_dt2 >= t_star)
n_test      <- nrow(test)
base_rate   <- mean(test$CARAVAN == pos_lab)

TP_cut <- cm_cut$table["yes","yes"]; FP_cut <- cm_cut$table["yes","no"]; FN_cut <- cm_cut$table["no","yes"]
TP_star <- cm_star$table["yes","yes"]; FP_star <- cm_star$table["yes","no"]; FN_star <- cm_star$table["no","yes"]

# Metrics
precision_cut  <- unname(cm_cut$byClass["Pos Pred Value"])
recall_cut     <- unname(cm_cut$byClass["Sensitivity"])
fpr_cut        <- unname(1 - cm_cut$byClass["Specificity"])
precision_star <- unname(cm_star$byClass["Pos Pred Value"])
recall_star    <- unname(cm_star$byClass["Sensitivity"])
fpr_star       <- unname(1 - cm_star$byClass["Specificity"])

# One combined table 
combined_tbl <- tibble(
  Parameter = c(
    "Threshold", "Mailed", "Precision", "Recall (TPR)", "FPR", "AUC", "Profit",
    "—",
    "Mail rate", "Base rate", "Lift (Precision/Base)",
    "TP (hits)", "FP (waste)", "FN (missed)",
    "Profit per mailed", "Δ Profit vs Cut off"
  ),
  `Cut off` = c(
    sprintf("%.6f", cutoff),
    fmt_int(mailed_cut),
    fmt_pct(precision_cut),
    fmt_pct(recall_cut),
    fmt_pct(fpr_cut),
    sprintf("%.3f", as.numeric(auc_dt2)),
    fmt_eur(profit_cut),
    "",
    fmt_pct(mailed_cut / n_test),
    fmt_pct(base_rate),
    sprintf("×%.2f", as.numeric(precision_cut) / as.numeric(base_rate)),
    fmt_int(TP_cut), fmt_int(FP_cut), fmt_int(FN_cut),
    fmt_eur(profit_cut / mailed_cut),
    "—"
  ),
  `*` = c(
    sprintf("%.6f", t_star),
    fmt_int(mailed_star),
    fmt_pct(precision_star),
    fmt_pct(recall_star),
    fmt_pct(fpr_star),
    sprintf("%.3f", as.numeric(auc_dt2)),  # AUC is threshold-independent
    fmt_eur(profit_star),
    "",
    fmt_pct(mailed_star / n_test),
    fmt_pct(base_rate),
    sprintf("×%.2f", as.numeric(precision_star) / as.numeric(base_rate)),
    fmt_int(TP_star), fmt_int(FP_star), fmt_int(FN_star),
    fmt_eur(profit_star / mailed_star),
    fmt_eur(profit_star - profit_cut)
  )
)

knitr::kable(combined_tbl, align = "lcc",
             caption = "Cut off vs. Profit-Maximizing (*) — Full summary")

```

## Decision Threshold Summary

-   **Business “Cut off” (0.0283)** — mails **everyone (100%)**
    (recall=1, specificity=0)\
    Precision **5.96%** (≈ base rate); **Profit €1,635.90**; **€0.94 per
    mail**.\
    **FN = 0**, but **FP waste = 1,642**.

-   **Profit-max “\*” (0.4221)** — mails **921 people (52.75%)**.\
    Precision **9.55%** (lift × **1.60**); Recall **84.62%** (**FN =
    16**);\
    **Profit €1,857.15** (**+13.5%** vs. cut off); **€2.02 per mail**
    (**+115%**).

-   **AUC = 0.721** for both → ranking quality is the same; the
    **threshold choice** drives the difference.

**Bottom line.** The business cutoff is too low for this (slightly
over-optimistic) tree, so it effectively says “mail everyone.” The
profit-max threshold cuts volume by \~47% while increasing total profit
and ROI per mail. **Use the profit-max (or a top-N) rule** going
forward.

# Decision Tree, Logistic Regression & Naive Bayes

Now, we will build logistic regression model, decision tree and naive
bayes

```{r DecTree_T3}
profit_per_conv <- 30; cost_mail <- 0.85
pos_lab <- "yes"

ths <- sort(unique(c(0, quantile(p_dt2, probs = seq(0, 1, 0.01)))))
profit_of <- function(t){
  contact <- p_dt2 >= t
  sum(ifelse(contact,
             ifelse(test$CARAVAN == pos_lab, profit_per_conv - cost_mail, -cost_mail),
             0))
}

profits <- sapply(ths, profit_of)
t_star  <- ths[which.max(profits)]

pred_star <- factor(ifelse(p_dt2 >= t_star, pos_lab, "no"),
                    levels = levels(test$CARAVAN))
cm_star <- caret::confusionMatrix(pred_star, test$CARAVAN, positive = pos_lab)

list(threshold = t_star,
     n_contacted = sum(p_dt2 >= t_star),
     precision   = cm_star$byClass["Pos Pred Value"],
     recall      = cm_star$byClass["Sensitivity"],
     fpr         = 1 - cm_star$byClass["Specificity"],
     profit      = max(profits))


```

We mail 921 people, catch \~85% of true buyers (high recall), but still
contact \~49% of non-buyers (FPR ≈ 0.493).

Precision ≈ 9.6% (vs base \~6%) good lift, but we can likely raise
precision (and keep profit) by (a) optimizing for top-N and (b)
calibrating tree probabilities (trees are famously over-confident).

```{r Fit models}

decTree_T3    <- partykit::ctree(fT3, data = training_set[, columns])
logitModel_T3 <- glm(fT3, family = binomial("logit"), data = training_set[, columns])
NBmodel_T3    <- naive_bayes(fT3, data = training_set[, columns])
LDAmodel_T3   <- lda(fT3, data = training_set[, columns])

```

```{r xx}
# Predict probabilities on test (needed for ROC)
p_dt <- predict(decTree_T3, newdata = test_set, type = "prob")[, 2]   # P(CARAVAN=1)
p_lr <- predict(logitModel_T3, newdata = test_set, type = "response") # P(CARAVAN=1)
p_nb <- predict(NBmodel_T3,  newdata = test_set, type = "prob")[, 2]  # P(CARAVAN=1)

# Actual labels
actual <- test_set$CARAVAN

# LDA probabilities for class "1" (fallback to 2nd column)
lda_out <- predict(LDAmodel_T3, newdata = test_set)
p_lda <- if (!is.null(colnames(lda_out$posterior)) && "1" %in% colnames(lda_out$posterior)) {
  lda_out$posterior[, "1"]
} else {
  lda_out$posterior[, 2]
}

```

```{r Plot ROC}
roc_decTree_T3 <- plot.roc(actual, p_dt,
  main = "ROC: decision tree (red), logistic reg (blue), NB (green), LDA (purple)",
  col = "red")
roc_logit_T3 <- lines.roc(actual, p_lr, col = "blue")
roc_NB_T3    <- lines.roc(actual, p_nb, col = "green")
lines.roc(actual, p_lda, col = "purple")

```

```{r Plot DTree}

plot(decTree_T3)

```

THIS DECISION TREE IS NOT LOOKING GOOD

```{r Confusion Matrix}

# Choose positive label (uses "1" if present; otherwise last level)
POS <- if ("1" %in% levels(test_set$CARAVAN)) "1" else tail(levels(test_set$CARAVAN), 1)
NEG <- setdiff(levels(test_set$CARAVAN), POS)[1]

to_class <- function(p) factor(ifelse(p >= 0.5, POS, NEG), levels = levels(test_set$CARAVAN))

pred_dt_class  <- to_class(p_dt)
pred_lr_class  <- to_class(p_lr)
pred_nb_class  <- to_class(p_nb)
pred_lda_class <- to_class(p_lda)

cm_dt  <- confusionMatrix(pred_dt_class,  test_set$CARAVAN, positive = POS)
cm_lr  <- confusionMatrix(pred_lr_class,  test_set$CARAVAN, positive = POS)
cm_nb  <- confusionMatrix(pred_nb_class,  test_set$CARAVAN, positive = POS)
cm_lda <- confusionMatrix(pred_lda_class, test_set$CARAVAN, positive = POS)



```

```{r Confusion Matrix Decision Tree}

cm_dt


```

THOSE VALUES ARE NOT LOOKING GOOD -\> I HAVE TO ADJUST THE DECISION TREE
MODEL

```{r Confusion Matrix Logistic Regression}

cm_lr


```

```{r Confusion Matrix Naive Bayes}

cm_nb


```

```{r Confusion Matrix LDA}

cm_lda


```

```{r Metrics}
# Metrics: Decision Tree
precision <- as.numeric(cm_dt$byClass["Pos Pred Value"])  # Precision
recall    <- as.numeric(cm_dt$byClass["Sensitivity"])     # Recall / TPR
fpr       <- 1 - as.numeric(cm_dt$byClass["Specificity"]) # FPR
f1        <- 2 * precision * recall / (precision + recall)
c(precision = precision, recall = recall, fpr = fpr, f1 = f1)

```

THERE IS SOMETHING WRONG

```{r Metrics}
# Metrics: Naive Bayes
precision <- as.numeric(cm_nb$byClass["Pos Pred Value"])
recall    <- as.numeric(cm_nb$byClass["Sensitivity"])
fpr       <- 1 - as.numeric(cm_nb$byClass["Specificity"])
f1        <- 2 * precision * recall / (precision + recall)
c(precision = precision, recall = recall, fpr = fpr, f1 = f1)


```

```{r Metrics}
# Metrics: LDA
precision <- as.numeric(cm_lda$byClass["Pos Pred Value"])
recall    <- as.numeric(cm_lda$byClass["Sensitivity"])
fpr       <- 1 - as.numeric(cm_lda$byClass["Specificity"])
f1        <- 2 * precision * recall / (precision + recall)
c(precision = precision, recall = recall, fpr = fpr, f1 = f1)


```

SOMETHING WRONG
